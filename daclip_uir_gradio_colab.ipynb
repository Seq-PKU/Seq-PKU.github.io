{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Seq-PKU/Seq-PKU.github.io/blob/master/daclip_uir_gradio_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VjYy0F2gZIPR",
        "outputId": "e674e35b-50b2-41f3-d238-6daa09894c54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "fatal: destination path 'daclip-uir' already exists and is not an empty directory.\n",
            "/content/daclip-uir\n",
            "aria2 is already the newest version (1.36.0-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 30 not upgraded.\n",
            "\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "929aa7|\u001b[1;32mOK\u001b[0m  |       0B/s|/content/daclip-uir/universal-image-restoration/config/daclip-sde/pretrained/daclip_ViT-B-32.pt\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "82e086|\u001b[1;32mOK\u001b[0m  |       0B/s|/content/daclip-uir/universal-image-restoration/config/daclip-sde/pretrained/universal-ir.pth\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-d8674981ba99>:13: DeprecationWarning: numpy.core is deprecated and has been renamed to numpy._core. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.multiarray.\n",
            "  torch.serialization.add_safe_globals([np.core.multiarray.scalar])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/daclip-uir/universal-image-restoration/config/daclip-sde\n",
            "/usr/local/lib/python3.11/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "export CUDA_VISIBLE_DEVICES=0\n",
            "OrderedDict([('name', 'Test'), ('mode', 'LQGT'), ('dataroot_GT', 'datasets/universal/deg_type/GT'), ('dataroot_LQ', 'datasets/universal/deg_type/LQ')])\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/daclip-uir/universal-image-restoration/config/daclip-sde/app.py\", line 29, in <module>\n",
            "    clip_model, preprocess = open_clip.create_model_from_pretrained('daclip_ViT-B-32', pretrained=opt['path']['daclip'])\n",
            "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/daclip-uir/universal-image-restoration/config/daclip-sde/../../open_clip/factory.py\", line 379, in create_model_from_pretrained\n",
            "    model = create_model(\n",
            "            ^^^^^^^^^^^^^\n",
            "  File \"/content/daclip-uir/universal-image-restoration/config/daclip-sde/../../open_clip/factory.py\", line 241, in create_model\n",
            "    load_checkpoint(model, checkpoint_path)\n",
            "  File \"/content/daclip-uir/universal-image-restoration/config/daclip-sde/../../open_clip/factory.py\", line 100, in load_checkpoint\n",
            "    state_dict = load_state_dict(checkpoint_path)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/daclip-uir/universal-image-restoration/config/daclip-sde/../../open_clip/factory.py\", line 89, in load_state_dict\n",
            "    checkpoint = torch.load(checkpoint_path, map_location=map_location)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/serialization.py\", line 1470, in load\n",
            "    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None\n",
            "_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
            "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
            "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
            "\tWeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([scalar])` or the `torch.serialization.safe_globals([scalar])` context manager to allowlist this global if you trust this class/function.\n",
            "\n",
            "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n"
          ]
        }
      ],
      "source": [
        "%cd /content\n",
        "!git clone -b gradio https://github.com/camenduru/daclip-uir\n",
        "%cd /content/daclip-uir\n",
        "\n",
        "!apt -y install -qq aria2\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/daclip-uir/resolve/main/daclip_ViT-B-32.pt -d /content/daclip-uir/universal-image-restoration/config/daclip-sde/pretrained -o daclip_ViT-B-32.pt\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/daclip-uir/resolve/main/universal-ir.pth -d /content/daclip-uir/universal-image-restoration/config/daclip-sde/pretrained -o universal-ir.pth\n",
        "\n",
        "!pip install -q einops ema-pytorch ftfy lmdb gradio\n",
        "import numpy as np\n",
        "import torch\n",
        "# 添加信任的全局对象到白名单\n",
        "torch.serialization.add_safe_globals([np.core.multiarray.scalar])\n",
        "%cd /content/daclip-uir/universal-image-restoration/config/daclip-sde\n",
        "!python app.py"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}